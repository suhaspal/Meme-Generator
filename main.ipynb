{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langchain tavily-python langgraph matplotlib langchain_community langchain-openai scikit-learn langchainhub langchain-ollama tiktoken langchain-nomic chromadb gpt4all firecrawl-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "local_llm = 'llama3.1'\n",
    "def get_llm():\n",
    "    return ChatOllama(model=local_llm, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are an assistant that will receiver two inputs. The first one is a prompt that a user gave to an AI meme-generator,\n",
    "    and the second input is a refined prompt created by an AI.\n",
    "    If you believe that the AI generated a prompt that is not related to the initial user prompt, take the user prompt and refine the prompt to make it\n",
    "    a one-sentence funny meme-like caption that references parts of the user query, or else jsut return the AI-Generated Query. Your return value should\n",
    "    contain no preamble or explanation.\n",
    "\n",
    "    User Query: {query}\n",
    "    \\n \\n\n",
    "    AI-Generated Query: {generated_query}\n",
    "    \\n \\n\n",
    "\n",
    "    Answer:\n",
    "    \"\"\",\n",
    "    input_variables=[\"query, generated_query\"]\n",
    ")\n",
    "\n",
    "llm = ChatOllama(model=local_llm, temperature=0)\n",
    "\n",
    "prompt_chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict, List\n",
    "from PIL import Image\n",
    "from IPython.display import Image, display\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from generate_memes import generate_image\n",
    "from generate_labels import generate_meme_label_wrapper\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "\n",
    "    user_prompt: str\n",
    "    generated_prompt: str\n",
    "    image_path: str\n",
    "\n",
    "def accept_prompt(state: AgentState) -> dict:\n",
    "    user_input = input(\"Give me some information on a meme you would like to see? \")\n",
    "    return {\"user_prompt\": user_input}\n",
    "\n",
    "def fix_prompt(state: AgentState) -> dict:\n",
    "    user_prompt = state['user_prompt']\n",
    "    distil_bert_prompt = generate_meme_label_wrapper(user_prompt)\n",
    "    result = prompt_chain.invoke({'query': user_prompt, 'generated_query': distil_bert_prompt})\n",
    "    \n",
    "    print(result)\n",
    "    return {\"generated_prompt\": result}\n",
    "    \n",
    "\n",
    "def make_image(state: AgentState) -> dict:\n",
    "    model_path = \"trained-comp-vis-model\"\n",
    "    output_dir = \"generated_memes\"\n",
    "    output_path = generate_image(\n",
    "        state['generated_prompt'], \n",
    "        model_path, \n",
    "        output_dir, \n",
    "        num_inference_steps=200,\n",
    "        guidance_scale=8.5,\n",
    "        width=768,\n",
    "        height=768\n",
    "    )\n",
    "    return {\"image_path\": output_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph \n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node('accept_prompt', accept_prompt)\n",
    "workflow.add_node('fix_prompt', fix_prompt)\n",
    "workflow.add_node('make_image', make_image)\n",
    "\n",
    "# Build Graph\n",
    "workflow.set_entry_point('accept_prompt')\n",
    "workflow.add_edge('accept_prompt', 'fix_prompt')\n",
    "workflow.add_edge('fix_prompt', 'make_image')\n",
    "workflow.add_edge('make_image', END)\n",
    "\n",
    "custom_graph = workflow.compile()\n",
    "display(Image(custom_graph.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_graph.invoke({'user_prompt': ''})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "memegenerator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
